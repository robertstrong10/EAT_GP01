{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c725ef0c",
   "metadata": {},
   "source": [
    "## Major Assignment 1 \n",
    "### Environmental Analysis Tools (ENEN90032) \n",
    "#### Authors: Olivia Borgstroem (1049030), Navindu de Silva (1084196), Robert Strong (1080043) \n",
    "#### Created: 22/08/2022\n",
    "#### Last Edited: 04/09/2022\n",
    "\n",
    "This is a notebook which complete the tasks outlined in 2022_ENEN90032_Assignment_01.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1b5048-3d27-428d-917e-63acefbf8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t\n",
    "import math\n",
    "# from tabulate import tabulate\n",
    "import statistics\n",
    "from pandas import Timestamp\n",
    "from cmath import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f3328",
   "metadata": {},
   "source": [
    "## Task 1 Exploratory Data Analysis - Meteorological Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310d5d9",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cba1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the csv files\n",
    "perth = pd.read_csv('Perth.csv')\n",
    "darwin = pd.read_csv('Darwin.csv')\n",
    "melbourne = pd.read_csv('Melbourne.csv')\n",
    "\n",
    "perth_rain = perth[\"Rainfall amount (millimetres)\"]\n",
    "darwin_rain = darwin[\"Rainfall amount (millimetres)\"]\n",
    "melbourne_rain = melbourne[\"Rainfall amount (millimetres)\"]\n",
    "\n",
    "# Use wet day daily rainfall and exclude values lower than 0.25mm detection limit\n",
    "perth_rain = perth_rain[perth_rain > 0.25]\n",
    "darwin_rain = darwin_rain[darwin_rain > 0.25]\n",
    "melbourne_rain = melbourne_rain[melbourne_rain > 0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0890e98",
   "metadata": {},
   "source": [
    "### Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa7b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sample means of all three locations \n",
    "pmean = np.mean(perth_rain)\n",
    "dmean = np.mean(darwin_rain)\n",
    "mmean = np.mean(melbourne_rain)\n",
    "\n",
    "# Find the median of all three locations\n",
    "pmed = np.median(perth_rain)\n",
    "dmed = np.median(darwin_rain)\n",
    "mmed = np.median(melbourne_rain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47abd8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find quartiles\n",
    "P25 = np.quantile(perth_rain, 0.25)\n",
    "P50 = np.quantile(perth_rain, 0.50)\n",
    "P75 = np.quantile(perth_rain, 0.75)\n",
    "\n",
    "D25 = np.quantile(darwin_rain, 0.25)\n",
    "D50 = np.quantile(darwin_rain, 0.50)\n",
    "D75 = np.quantile(darwin_rain, 0.75)\n",
    "\n",
    "M25 = np.quantile(melbourne_rain, 0.25)\n",
    "M50 = np.quantile(melbourne_rain, 0.50)\n",
    "M75 = np.quantile(melbourne_rain, 0.75)\n",
    "\n",
    "# Find the trimean of all three locations\n",
    "Ptrimean = (P25 + 2 * P50 + P75)/4\n",
    "Dtrimean = (D25 + 2 * D50 + D75)/4\n",
    "Mtrimean = (M25 + 2 * M50 + M75)/4\n",
    "\n",
    "# Find the IQR of all three locations\n",
    "PIQR = (P75 - P25)\n",
    "DIQR = (D75 - D25)\n",
    "MIQR = (M75 - M25)\n",
    "\n",
    "# Find the sample standard deviation of all three locations#\n",
    "PSD = statistics.stdev(perth_rain)\n",
    "DSD = statistics.stdev(darwin_rain)\n",
    "MSD = statistics.stdev(melbourne_rain)\n",
    "\n",
    "# Find the median absolute deviation of all three locations#\n",
    "PMAD = stats.median_abs_deviation(perth_rain)\n",
    "DMAD = stats.median_abs_deviation(darwin_rain)\n",
    "MMAD = stats.median_abs_deviation(melbourne_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3cb293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabulate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Put them into a table\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m table \u001b[39m=\u001b[39m [[\u001b[39m'\u001b[39m\u001b[39mItem\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPerth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDarwin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMelbourne\u001b[39m\u001b[39m'\u001b[39m], [\u001b[39m'\u001b[39m\u001b[39mSample mean\u001b[39m\u001b[39m'\u001b[39m, pmean, dmean, mmean],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m          [\u001b[39m'\u001b[39m\u001b[39mMedian\u001b[39m\u001b[39m'\u001b[39m, pmed, dmed, mmed], [\u001b[39m'\u001b[39m\u001b[39mTrimean\u001b[39m\u001b[39m'\u001b[39m, Ptrimean, Dtrimean,Mtrimean],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         [\u001b[39m'\u001b[39m\u001b[39mSample standard deviation\u001b[39m\u001b[39m'\u001b[39m, PSD, DSD,MSD],[\u001b[39m'\u001b[39m\u001b[39mIQR\u001b[39m\u001b[39m'\u001b[39m, PIQR, DIQR, MIQR],[\u001b[39m'\u001b[39m\u001b[39mMedian absolute deviation\u001b[39m\u001b[39m'\u001b[39m, PMAD,DMAD,MMAD],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         [\u001b[39m'\u001b[39m\u001b[39mSample skewness\u001b[39m\u001b[39m'\u001b[39m, Pskew, Dskew, Mskew],[\u001b[39m'\u001b[39m\u001b[39mYule-Kendall Index\u001b[39m\u001b[39m'\u001b[39m, PYK, DYK, MYK]]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/RobStrong/Desktop/EAT/Ass1/EAT_GP01/GP01_Assignment1_FINAL.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(tabulate(table))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tabulate' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample skewness of all three locations\n",
    "Pskew = float(stats.skew(perth_rain))\n",
    "Dskew = float(stats.skew(darwin_rain))\n",
    "Mskew = float(stats.skew(melbourne_rain))\n",
    "\n",
    "# Yule-Kendall index of all three locations\n",
    "PYK = (P25 + P75- (2*P50))/(P75 - P25)\n",
    "DYK = (D25 + D75- (2*D50))/(D75 - D25)\n",
    "MYK = (M25 + M75- (2*M50))/(M75 - M25)\n",
    "\n",
    "# Put them into a table\n",
    "table = [['Item', 'Perth', 'Darwin', 'Melbourne'], ['Sample mean', pmean, dmean, mmean],\n",
    "         ['Median', pmed, dmed, mmed], ['Trimean', Ptrimean, Dtrimean,Mtrimean],\n",
    "        ['Sample standard deviation', PSD, DSD,MSD],['IQR', PIQR, DIQR, MIQR],['Median absolute deviation', PMAD,DMAD,MMAD],\n",
    "        ['Sample skewness', Pskew, Dskew, Mskew],['Yule-Kendall Index', PYK, DYK, MYK]]\n",
    "print(tabulate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3868aa7",
   "metadata": {},
   "source": [
    "### Task 1.2 (and Task 1.3 done at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the wet-day daily rainfall data, fit i) a Gaussian, ii) a gamma, and iii) a Weibull2 distribution functions (also do Task 1.3) \n",
    "\n",
    "# For Perth\n",
    "optbin = 2.1 * (P75 - P25 ) / (len(perth_rain)**(1/3))\n",
    "print('Optimal bin size (rule of thumb) =', optbin)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(perth_rain, bins = np.arange(np.min(perth_rain), np.max(perth_rain) + optbin, optbin), density = True, histtype = 'bar', align = 'left')\n",
    "plt.xlabel('Daily rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Probability', fontsize=16)\n",
    "plt.title('Daily Rainfall in Perth', fontsize=16)\n",
    "\n",
    "# Gaussian PDF fitted to the data\n",
    "x = list(range(0, 40, 1)) #range of x axis values - should be same as the perth_rain \n",
    "pdf = stats.norm.pdf(x, loc = np.mean(perth_rain), scale = np.std(perth_rain))\n",
    "plt.plot(x, pdf, label = 'Gaussian PDF', lw = 2, c = 'r')\n",
    "\n",
    "# Gamma PDF - bounded by low values  \n",
    "shape_g, loc, scale_g = stats.gamma.fit(perth_rain) #loc is only needed if you need to shift the distribution\n",
    "#print(shape, loc, scale) \n",
    "pdf = stats.gamma.pdf(x, a = shape_g, scale = scale_g) #a is alpha and scale is 1/beta\n",
    "plt.plot(x, pdf, label = 'Gamma PDF', lw = 2, c = 'y')\n",
    "\n",
    "# Weibull PDF - bound by minimum\n",
    "c = perth_rain\n",
    "mean, var, skew = stats.weibull_min.fit(c)\n",
    "plt.plot(x, stats.weibull_min.pdf(x, mean, var, skew),'g', lw=2, alpha=0.6, label='Weibull PDF')\n",
    "\n",
    "# Generalised Pareto fit (Task 1.3)\n",
    "c = perth_rain\n",
    "mean, var, skew = stats.genpareto.fit(c)\n",
    "plt.plot(x, stats.genpareto.pdf(x,mean, var, skew),'b', lw=2, alpha=0.5, label='Generalised Pareto PDF')\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.savefig('1_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of empirical CDFs for Perth rainfall\n",
    "\n",
    "def ecdf(perth_rain):\n",
    "\n",
    "    # convert data to a numpy array\n",
    "    data = np.atleast_1d(perth_rain) # Scalar inputs are converted to 1-dimensional arrays, whilst higher-dimensional inputs are preserved.\n",
    "\n",
    "    # find the unique values and their corresponding frequency or counts\n",
    "    loc, counts = np.unique(perth_rain, return_counts=True) #https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
    "\n",
    "    # now converting the freq to ecdf\n",
    "    cum_prob = np.cumsum(counts).astype(np.double) / data.size\n",
    "\n",
    "    return loc, cum_prob\n",
    "\n",
    "a,b = ecdf(perth_rain)\n",
    "\n",
    "# Plotting CDFs \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(a, b, lw=1.5, color='g', label='ECDF') #plot ecdf\n",
    "plt.plot(a, stats.norm.cdf(a, loc = np.mean(perth_rain), scale = np.std(perth_rain)), lw=2, color='r', label='Gaussian CDF') # Gaussian CDF\n",
    "plt.plot(a, stats.gamma.cdf(a, a = shape_g, scale = scale_g), lw=2, color='b', label='Gamma CDF') # Gamma CDF\n",
    "plt.plot(x, stats.weibull_min.cdf(x, mean, var, skew),'g', lw=2, alpha=0.6, label='Weibull CDF') # Weibull CDF\n",
    "plt.plot(x, stats.genpareto.cdf(x,mean, var, skew),'y', lw=2, alpha=0.5, label='Generalised Pareto CDF') # Generalised Pareto\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.title('Comparison of CDFs for Perth Rainfall', fontsize=18)\n",
    "plt.xlabel('Rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Cumulative Probability', fontsize=16)\n",
    "plt.savefig('2_P.png', dpi=360)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab13825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Q plot for evaluation for Perth\n",
    "\n",
    "# A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. \n",
    "# If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. \n",
    "\n",
    "n = 100\n",
    "Q_data = np.empty((n,1)) * np.nan # Quantiles in data \n",
    "\n",
    "\n",
    "Q_gamma = np.empty((n,1)) * np.nan # Quantiles in Gamma distribution\n",
    "Q_gauss = np.empty((n,1)) * np.nan # Quantiles in Gasssian distribution\n",
    "Q_weibull = np.empty((n,1)) * np.nan # Quantiles in Gasssian distribution\n",
    "Q_genpareto = np.empty((n,1)) * np.nan # Task 1.3 Quantiles in Generalised Pareto distribuition\n",
    "\n",
    "# Create samples from Gaussian, Gamma and Weibull (and General Pareto) distribution\n",
    "gauss_data = np.random.normal(loc=np.mean(perth_rain), scale=np.std(perth_rain), size=10000)\n",
    "gamma_data = np.random.gamma(shape_g, scale=scale_g, size=10000)  # Mean and std of the distribution\n",
    "weibull_data = np.random.weibull(mean, size=10000)\n",
    "genpareto_data = stats.genpareto.rvs(mean, size=10000)\n",
    "\n",
    "for i in range(1, n+1, 1):\n",
    "    p = (i - 1/3) / (n + 1/3) # Plotting location\n",
    "    Q_data[i-1] = np.quantile(perth_rain, p)\n",
    "    Q_gamma[i-1] = np.quantile(gamma_data, p)\n",
    "    Q_gauss[i-1] = np.quantile(gauss_data, p)\n",
    "    Q_weibull[i-1] = np.quantile(weibull_data, p)\n",
    "    Q_genpareto[i-1] = np.quantile(genpareto_data, p)\n",
    "    \n",
    "# Plotting\n",
    "print('Shape of data used for Q-Q plot =', Q_gamma.shape); print(' ')\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(Q_gauss , Q_data, alpha = 0.5,label='Gaussian')\n",
    "plt.scatter(Q_gamma , Q_data, alpha = 0.3,label='Gamma')\n",
    "plt.scatter(Q_weibull , Q_data, alpha = 0.6,label='Weibull')\n",
    "plt.scatter(Q_genpareto , Q_data, alpha = 0.6,label='Generalised Pareto')\n",
    "plt.plot([-15, 50], [-15, 50], '--k', label = '1:1 line')\n",
    "plt.legend(loc=0, fontsize = 14)\n",
    "plt.title('Q-Q Plot for Perth rainfall', fontsize=18)\n",
    "plt.xticks(np.arange(0,51,10), fontsize = 14)\n",
    "plt.yticks(np.arange(0,51,10), fontsize = 14)\n",
    "plt.xlabel('Theoretical Quantiles', fontsize=16)\n",
    "plt.ylabel('Data Quantiles', fontsize=16)\n",
    "plt.savefig('3_P.png', dpi=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7af871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the wet-day daily rainfall data, fit i) a Gaussian, ii) a gamma, and iii) a Weibull distribution functions \n",
    "# Darwin\n",
    "\n",
    "# Optimal Bin formula\n",
    "optbin = 2.1 * (D75 - D25 ) / (len(darwin_rain)**(1/3)) \n",
    "print('Optimal bin size (rule of thumb) =', optbin)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(darwin_rain, bins = np.arange(np.min(darwin_rain), np.max(darwin_rain) + optbin, optbin), density = True, histtype = 'bar', align = 'left')\n",
    "plt.xlabel('Daily rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Probability', fontsize=16)\n",
    "plt.title('Daily Rainfall in Darwin', fontsize=16)\n",
    "\n",
    "# Gaussian PDF fitted to the data\n",
    "x = list(range(0, 112, 1)) #range of x axis values - should be same as the darwin_rain \n",
    "pdf = stats.norm.pdf(x, loc = np.mean(darwin_rain), scale = np.std(darwin_rain))\n",
    "\n",
    "# Gamma PDF - bounded by low values  \n",
    "shape_g, loc, scale_g = stats.gamma.fit(darwin_rain)\n",
    "pdf = stats.gamma.pdf(x, a = shape_g, scale = scale_g) #a is alpha and scale is 1/beta\n",
    "\n",
    "# Weibull PDF - bound by minimum\n",
    "c = darwin_rain\n",
    "mean, var, skew = stats.weibull_min.fit(c)\n",
    "\n",
    "# Generalised Pareto fit (Task 1.3)\n",
    "c = darwin_rain\n",
    "mean, var, skew = stats.genpareto.fit(c)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, pdf, label = 'Gaussian PDF', lw = 2, c = 'r')\n",
    "plt.plot(x, pdf, label = 'Gamma PDF', lw = 2, c = 'y')\n",
    "plt.plot(x, stats.weibull_min.pdf(x, mean, var, skew),'g', lw=2, alpha=0.6, label='Weibull PDF')\n",
    "plt.plot(x, stats.genpareto.pdf(x,mean, var, skew),'b', lw=2, alpha=0.5, label='Generalised Pareto PDF')\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.savefig('1_D.png', dpi=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of empirical CDFs for Darwin rainfall\n",
    "\n",
    "def ecdf(darwin_rain):\n",
    "\n",
    "    # Convert data to a numpy array\n",
    "    data = np.atleast_1d(darwin_rain) # Scalar inputs are converted to 1-dimensional arrays, whilst higher-dimensional inputs are preserved.\n",
    "\n",
    "    # Find the unique values and their corresponding frequency or counts\n",
    "    loc, counts = np.unique(darwin_rain, return_counts=True) # https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
    "\n",
    "    # Converting the freq to ecdf\n",
    "    cum_prob = np.cumsum(counts).astype(np.double) / data.size\n",
    "\n",
    "    return loc, cum_prob\n",
    "\n",
    "a,b = ecdf(darwin_rain)\n",
    "\n",
    "# Plotting CDFs \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(a, b, lw=2, color='g', label='ECDF') #plot ecdf\n",
    "plt.plot(a, stats.norm.cdf(a, loc = np.mean(darwin_rain), scale = np.std(darwin_rain)), lw=2, color='r', label='Gaussian CDF') #plot Gaussian cdf\n",
    "plt.plot(a, stats.gamma.cdf(a, a = shape_g, scale = scale_g), lw=2, color='b', label='Gamma CDF') #plot Gamma cdf\n",
    "plt.plot(x, stats.weibull_min.cdf(x, mean, var, skew),'g', lw=2, alpha=0.6, label='Weibull CDF')\n",
    "plt.plot(x, stats.genpareto.cdf(x,mean, var, skew),'y', lw=2, alpha=0.5, label='Generalised Pareto CDF') #Task 1.3\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.title('Comparison of CDFs for Darwin Rainfall', fontsize=18)\n",
    "plt.xlabel('Rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Cumulative Probability', fontsize=16)\n",
    "plt.savefig('2_D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c98f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot for evaluation for Darwin\n",
    "n = 100\n",
    "Q_data = np.empty((n,1)) * np.nan # Quantiles in data \n",
    "Q_gamma = np.empty((n,1)) * np.nan # Quantiles in Gamma distribution\n",
    "Q_gauss = np.empty((n,1)) * np.nan # Quantiles in Gasssian distribution\n",
    "Q_weibull = np.empty((n,1)) * np.nan # Quantiles in Gasssian distribution\n",
    "Q_genpareto = np.empty((n,1)) * np.nan # Task 1.3 quantiles in Generalised Pareto distribuition\n",
    "\n",
    "# Create samples from Gaussian, Gamma and Weibull distribution\n",
    "gauss_data = np.random.normal(loc=np.mean(darwin_rain), scale=np.std(darwin_rain), size=10000)\n",
    "gamma_data = np.random.gamma(shape_g, scale=scale_g, size=10000)  # Mean and std of the dist\n",
    "weibull_data = np.random.weibull(mean, size=10000)\n",
    "genpareto_data = stats.genpareto.rvs(mean, size=10000) #(Task 1.3)\n",
    "\n",
    "for i in range(1, n+1, 1):\n",
    "    p = (i - 1/3) / (n + 1/3) # Plotting location\n",
    "    Q_data[i-1] = np.quantile(darwin_rain, p)\n",
    "    Q_gamma[i-1] = np.quantile(gamma_data, p)\n",
    "    Q_gauss[i-1] = np.quantile(gauss_data, p)\n",
    "    Q_weibull[i-1] = np.quantile(weibull_data, p)\n",
    "\n",
    "print('Shape of data used for Q-Q plot =', Q_gamma.shape); print(' ')\n",
    "\n",
    "# Plotting Q-Q\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(Q_gauss , Q_data, alpha = 0.5,label='Gaussian')\n",
    "plt.scatter(Q_gamma , Q_data, alpha = 0.3,label='Gamma')\n",
    "plt.scatter(Q_weibull , Q_data, alpha = 0.6,label='Weibull')\n",
    "plt.scatter(Q_genpareto , Q_data, alpha = 0.6,label='Generalised Pareto')\n",
    "plt.plot([-15, 120], [-15, 120], '--k', label = '1:1 line')\n",
    "plt.legend(loc=4, fontsize = 14)\n",
    "plt.title('Q-Q Plot for Darwin rainfall', fontsize=18)\n",
    "plt.xticks(np.arange(0,121,20), fontsize = 14)\n",
    "plt.yticks(np.arange(0,121,20), fontsize = 14)\n",
    "plt.xlabel('Theoretical Quantiles', fontsize=16)\n",
    "plt.ylabel('Data Quantiles', fontsize=16)\n",
    "plt.savefig('3_D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33464cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the wet-day daily rainfall data, fit i) a Gaussian, ii) a gamma, and iii) a Weibull2 distribution functions \n",
    "# Melbourne \n",
    "\n",
    "# Optimal Bin formula\n",
    "optbin = 2.1 * (M75 - M25 ) / (len(melbourne_rain)**(1/3)) \n",
    "print('Optimal bin size (rule of thumb) =', optbin)\n",
    "plt.figure()\n",
    "plt.hist(darwin_rain, bins = np.arange(np.min(melbourne_rain), np.max(melbourne_rain) + optbin, optbin), density = True, histtype = 'bar', align = 'left')\n",
    "plt.xlabel('Daily rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Probability', fontsize=16)\n",
    "plt.title('Daily Rainfall in Melbourne', fontsize=16)\n",
    "\n",
    "# Gaussian PDF fitted to the data\n",
    "x = list(range(0, 60, 1)) #range of x axis values - should be same as the melbourne_rain \n",
    "pdf = stats.norm.pdf(x, loc = np.mean(melbourne_rain), scale = np.std(melbourne_rain))\n",
    "\n",
    "# Gamma PDF - bounded by low values  \n",
    "shape_g, loc, scale_g = stats.gamma.fit(melbourne_rain) #loc is only needed if you need to shift the distribution\n",
    "pdf = stats.gamma.pdf(x, a = shape_g, scale = scale_g) #a is alpha a\n",
    "\n",
    "# Weibull PDF - bound by minimum\n",
    "c = melbourne_rain\n",
    "mean, var, skew = stats.weibull_min.fit(c)\n",
    "\n",
    "# Generalised Pareto fit (Task 1.3)\n",
    "c = melbourne_rain\n",
    "mean, var, skew = stats.genpareto.fit(c)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x, stats.genpareto.pdf(x,mean, var, skew),'b', lw=3, alpha=0.5, label='Generalised Pareto PDF')\n",
    "plt.plot(x, pdf, label = 'Gaussian PDF', lw = 4, c = 'r')\n",
    "plt.plot(x, pdf, label = 'Gamma PDF', lw = 4, c = 'y')\n",
    "plt.plot(x, stats.weibull_min.pdf(x, mean, var, skew),'g', lw=4, alpha=0.6, label='Weibull PDF')\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.savefig('1_M')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of empirical CDFS for Melbourne rainfall\n",
    "\n",
    "def ecdf(melbourne_rain):\n",
    "\n",
    "    # Convert data to a numpy array\n",
    "    data = np.atleast_1d(melbourne_rain) # Scalar inputs are converted to 1-dimensional arrays, whilst higher-dimensional inputs are preserved.\n",
    "\n",
    "    # Find the unique values and their corresponding frequency or counts\n",
    "    loc, counts = np.unique(melbourne_rain, return_counts=True) # https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
    "\n",
    "    # Converting the freq to ecdf\n",
    "    cum_prob = np.cumsum(counts).astype(np.double) / data.size\n",
    "\n",
    "    return loc, cum_prob\n",
    "\n",
    "a,b = ecdf(melbourne_rain)\n",
    "\n",
    "# Plotting CDFs \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(a, b, lw=1.5, color='g', label='ECDF') #plot ecdf\n",
    "plt.plot(a, stats.norm.cdf(a, loc = np.mean(melbourne_rain), scale = np.std(melbourne_rain)), lw=1.5, color='r', label='Gaussian CDF') #plot Gaussian cdf\n",
    "plt.plot(a, stats.gamma.cdf(a, a = shape_g, scale = scale_g), lw=1.5, color='b', label='Gamma CDF') #plot Gamma cdf\n",
    "plt.plot(x, stats.weibull_min.cdf(x, mean, var, skew),'g', lw=4, alpha=0.6, label='Weibull CDF')\n",
    "plt.plot(x, stats.genpareto.cdf(x,mean, var, skew),'y', lw=3, alpha=0.5, label='Generalised Pareto CDF') #Task 1.3\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.title('Comparison of CDFs for Melbourne Rainfall', fontsize=18)\n",
    "plt.xlabel('Rainfall (mm)', fontsize=16)\n",
    "plt.ylabel('Cumulative Probability', fontsize=16)\n",
    "plt.savefig('2_M')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Q plot for evaluation for Melbourne\n",
    "\n",
    "n = 100 \n",
    "Q_data = np.empty((n,1)) * np.nan # Quantiles in data \n",
    "Q_gamma = np.empty((n,1)) * np.nan # Quantiles in Gamma distribution\n",
    "Q_gauss = np.empty((n,1)) * np.nan # Quantiles in Gasssian distribution\n",
    "Q_weibull = np.empty((n,1)) * np.nan #Quantiles in Gasssian distribution\n",
    "Q_genpareto = np.empty((n,1)) * np.nan # Task 1.3 quantiles in Generalised Pareto distribuition\n",
    "\n",
    "# Create samples from Gaussian, Gamma and Weibull distribution\n",
    "gauss_data = np.random.normal(loc=np.mean(melbourne_rain), scale=np.std(melbourne_rain), size=10000)\n",
    "gamma_data = np.random.gamma(shape_g, scale=scale_g, size=10000)  # Mean and std of the dist\n",
    "weibull_data = np.random.weibull(mean, size=10000)\n",
    "genpareto_data = stats.genpareto.rvs(mean, size=10000) # Task 1.3\n",
    "\n",
    "for i in range(1, n+1, 1):\n",
    "    p = (i - 1/3) / (n + 1/3) # Plotting location\n",
    "    Q_data[i-1] = np.quantile(melbourne_rain, p)\n",
    "    Q_gamma[i-1] = np.quantile(gamma_data, p)\n",
    "    Q_gauss[i-1] = np.quantile(gauss_data, p)\n",
    "    Q_weibull[i-1] = np.quantile(weibull_data, p)\n",
    "\n",
    "# Plotting Q-Q\n",
    "print('Shape of data used for Q-Q plot =', Q_gamma.shape); print(' ')\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(Q_gauss , Q_data, alpha = 0.5,label='Gaussian')\n",
    "plt.scatter(Q_gamma , Q_data, alpha = 0.3,label='Gamma')\n",
    "plt.scatter(Q_weibull , Q_data, alpha = 0.6,label='Weibull')\n",
    "plt.scatter(Q_genpareto , Q_data, alpha = 0.6,label='Generalised Pareto')\n",
    "plt.plot([-10, 60], [-10, 60], '--k', label = '1:1 line')\n",
    "plt.legend(loc=4, fontsize = 14)\n",
    "plt.title('Q-Q Plot for Melbourne rainfall', fontsize=18)\n",
    "plt.xticks(np.arange(0,61,10), fontsize = 14)\n",
    "plt.yticks(np.arange(0,61,10), fontsize = 14)\n",
    "plt.xlabel('Theoretical Quantiles', fontsize=16)\n",
    "plt.ylabel('Data Quantiles', fontsize=16)\n",
    "plt.savefig('3_M')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993852b",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "Generalised Pareto fit completed alongside Task 1.2 (above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb160d",
   "metadata": {},
   "source": [
    "### Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbe50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the log-likelihood values of the PERTH fits\n",
    "\n",
    "Gamma_LogL = stats.gamma.logpdf(perth_rain, a = shape_g, scale = scale_g).sum()\n",
    "print('Gamma negative loglikelihood = ', -Gamma_LogL)\n",
    "\n",
    "Gaussian_LogL = stats.norm.logpdf(perth_rain, np.mean(perth_rain), np.std(perth_rain)).sum()\n",
    "print('Gaussian negative loglikelihood = ' ,-Gaussian_LogL)\n",
    "\n",
    "Weibull_LogL = stats.weibull_min.logpdf(perth_rain, mean).sum()\n",
    "print('Weibull negative loglikelihood = ' ,-Weibull_LogL)\n",
    "\n",
    "Genpareto_LogL = stats.genpareto.logpdf(perth_rain, mean).sum()\n",
    "print('Gen Pareto negative loglikelihood = ' ,-Genpareto_LogL)\n",
    "\n",
    "# Maximizing log likelihood is equivalent to minimizing \"negative log likelihood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the log-likelihood values of the DARWIN fits\n",
    "\n",
    "Gamma_LogL = stats.gamma.logpdf(darwin_rain, a = shape_g, scale = scale_g).sum()\n",
    "print('Gamma negative loglikelihood = ', -Gamma_LogL)\n",
    "\n",
    "Gaussian_LogL = stats.norm.logpdf(darwin_rain, np.mean(darwin_rain), np.std(darwin_rain)).sum()\n",
    "print('Gaussian negative loglikelihood = ' ,-Gaussian_LogL)\n",
    "\n",
    "Weibull_LogL = stats.weibull_min.logpdf(darwin_rain, mean).sum()\n",
    "print('Weibull negative loglikelihood = ' ,-Weibull_LogL)\n",
    "\n",
    "Genpareto_LogL = stats.genpareto.logpdf(darwin_rain, mean).sum()\n",
    "print('Gen Pareto negative loglikelihood = ' ,-Genpareto_LogL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the log-likelihood values of the MELBOURNE fits\n",
    "\n",
    "Gamma_LogL = stats.gamma.logpdf(melbourne_rain, a = shape_g, scale = scale_g).sum()\n",
    "print('Gamma negative loglikelihood = ', -Gamma_LogL)\n",
    "\n",
    "Gaussian_LogL = stats.norm.logpdf(melbourne_rain, np.mean(melbourne_rain), np.std(melbourne_rain)).sum()\n",
    "print('Gaussian negative loglikelihood = ' ,-Gaussian_LogL)\n",
    "\n",
    "Weibull_LogL = stats.weibull_min.logpdf(melbourne_rain, mean).sum()\n",
    "print('Weibull negative loglikelihood = ' ,-Weibull_LogL)\n",
    "\n",
    "Genpareto_LogL = stats.genpareto.logpdf(melbourne_rain, mean).sum()\n",
    "print('Gen Pareto negative loglikelihood = ' ,-Genpareto_LogL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498e97e",
   "metadata": {},
   "source": [
    "## Task 2 Daily Max Temperature at Melbourne and Essendon Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in csv\n",
    "tullamarine = pd.read_csv('Tullamarine.csv')\n",
    "essendon = pd.read_csv('Essendon.csv')\n",
    "\n",
    "tullamarine = tullamarine[\"Maximum temperature (Degree C)\"]\n",
    "essendon = essendon[\"Maximum temperature (Degree C)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda192a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are NaN values \n",
    "print('Any Tulla NaN values? (True/False):', tullamarine.isnull().values.any()) #check nan values\n",
    "print('Any Essen NaN values? (True/False):', essendon.isnull().values.any()) #check nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b00b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframe with both sites\n",
    "df1 = pd.DataFrame(tullamarine)\n",
    "df2 = pd.DataFrame(essendon)\n",
    "\n",
    "df1.rename(columns ={df1.columns[0]:\"Tullamarine\"}, inplace = True)\n",
    "df2.rename(columns ={df2.columns[0]:\"Essendon\"}, inplace = True)\n",
    "\n",
    "data = pd.concat([df1, df2], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "header = list(data.columns) #col name\n",
    "plt.figure(figsize=(8,8))\n",
    "boxprop = dict(linestyle='-', linewidth=1.5)\n",
    "medianprop = dict(linestyle='-', linewidth=1.5)\n",
    "medianprop = dict(linestyle='-', linewidth=1.5)\n",
    "data.boxplot(column=header, labels=header, grid=False, fontsize=16, \n",
    "boxprops=boxprop,medianprops=medianprop, whiskerprops = medianprop, capprops = medianprop, showfliers=True)\n",
    "plt.ylabel('Temperature (C)', fontsize=16)\n",
    "plt.title('Boxplot of Maximum Temperature in Tullamarine and Essendon', fontsize=16)\n",
    "plt.savefig('boxplot_.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## p-value based method for comparison of means \n",
    "\n",
    "# Calculate the difference between the sample means\n",
    "mean_diff = np.mean(data['Tullamarine']) - np.mean(data['Essendon'])  # mean(tullamarine) - mean(essendon)\n",
    "print('Difference between two sample mean values =', mean_diff)\n",
    "\n",
    "# Calculate the std \n",
    "s1 = np.std(data['Tullamarine'],ddof=1)\n",
    "s2 = np.std(data['Essendon'],ddof=1)\n",
    "print('Stdev1 =', s1, ', Stdev2 =', s2)\n",
    "\n",
    "# The size of each group \n",
    "n = 365 # Size of both lists are the same therefore n1 = n2\n",
    "sdPooled = np.sqrt(((n - 1)*s1**2 + (n-1)*s2**2)/(n+n-2))\n",
    "SE = sdPooled * np.sqrt(1/n + 1/n)\n",
    "print('Pooled SD = ', sdPooled)\n",
    "print('Standard Error = ', SE)\n",
    "\n",
    "t_stat = mean_diff/SE\n",
    "print('t-value =', t_stat)\n",
    "t_dist = stats.t(df = n+n - 2) # Create a t-table of degree of freedom\n",
    "pval = t_dist.cdf(t_stat)\n",
    "print('p-value =',pval*2) # 2-sided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbf31b",
   "metadata": {},
   "source": [
    "## Task 3 Newcomb-Michelson Velocity of Light Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in txt data\n",
    "light_df = pd.read_csv('NewcombLight.txt', header=None)\n",
    "\n",
    "# Constants\n",
    "distance = 7442 # m at sea level\n",
    "wikipedia_speed = 299792458 # m/s accessed from https://en.wikipedia.org/wiki/Speed_of_light\n",
    "\n",
    "light_df = distance/ light_df\n",
    "light_df.columns = [\"speed\"] \n",
    "light_mean = np.mean(light_df.speed)\n",
    "\n",
    "# T-test\n",
    "light_t = stats.ttest_1samp(light_df, wikipedia_speed, alternative='two-sided', )\n",
    "print(\"Sample Mean = {} m/s\".format(light_mean))\n",
    "print(\"T-test pvalue = {}\".format(light_t.pvalue[0])) \n",
    "\n",
    "# Bootstrap\n",
    "light_b_data = (light_df,)\n",
    "light_bootstrap_ci = stats.bootstrap(light_b_data, np.mean, confidence_level=0.99, random_state=1, method='percentile')\n",
    "bootstrap_mean = (light_bootstrap_ci.confidence_interval[0][0]+light_bootstrap_ci.confidence_interval[1][0])/2\n",
    "print(\"99% CI from Bootstrap: ({}, {})\".format(light_bootstrap_ci.confidence_interval[0][0], light_bootstrap_ci.confidence_interval[1][0]))\n",
    "print(\"Mean from Bootstraps = {} m/s\".format(bootstrap_mean))\n",
    "\n",
    "print(\"Speed of Light (Wikipedia, 2022) = {} m/s\".format(wikipedia_speed))\n",
    "print(\"Percentage Error Sample Mean and Wikipedia = {}%\".format(abs((light_mean-wikipedia_speed)/light_mean *100)))\n",
    "print(\"Percentage Error q0.5 Bootstrap and Wikipedia = {}%\".format(\n",
    "    abs(((light_bootstrap_ci.confidence_interval[0][0] +\n",
    "    light_bootstrap_ci.confidence_interval[1][0])/2-wikipedia_speed)\n",
    "    /(light_bootstrap_ci.confidence_interval[0][0] \n",
    "    + light_bootstrap_ci.confidence_interval[1][0])/2 *100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc55d76",
   "metadata": {},
   "source": [
    "## Task 4 Space Shuttle O-Ring Failures\n",
    "Hypothesised that launch temperatures under 65F (cool launches) resulted in more failures than when launch temperatures were above 65F (warm launches). \\\n",
    "Alternative hypothesis: mean(cool)-mean(warm)>0 \\\n",
    "Null hypothesis: mean(cool)-mean(warm)≤0\n",
    "\n",
    "Hence, one-sided two-sample t-test comparing the two temperature groups. Only want one-sided test as interested in if cool conditions increased failure risk, not the reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0da4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read launch temperature data and check for NaN and incorrect values\n",
    "launch_temps = pd.read_excel(\"O_Ring_Data.XLS\")\n",
    "print('Any temp NaN values? (True/False):', launch_temps.isnull().values.any())  # check nan values\n",
    "\n",
    "allowed_groups = ['COOL', 'WARM']\n",
    "print('Only allowed LAUNCH group names? (True/False):', \n",
    "      all(s in allowed_groups for s in launch_temps.loc[:, 'LAUNCH']))  # check allowed temperature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f327e0-ed8b-4789-881a-4823eccbf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into the two groups + get mean\n",
    "cool = launch_temps[np.logical_not(launch_temps.loc[:,'LAUNCH']!='COOL')]\n",
    "cool = cool.loc[:, 'INCIDENTS']\n",
    "cool_mean = np.mean(cool)\n",
    "print('Mean number of accidents for cool launches:', cool_mean)\n",
    "\n",
    "warm = launch_temps[np.logical_not(launch_temps.loc[:,'LAUNCH']!='WARM')]\n",
    "warm = warm.loc[:, 'INCIDENTS'].reset_index(drop=True)\n",
    "warm_mean = np.mean(warm)\n",
    "print('Mean number of accidents for warm launches:', warm_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b439dfd-9366-41a3-bab5-f7f815e33d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'greater' indicates one-sided test expecting frequency of cool indicents > warm incidents\n",
    "t_stat = stats.ttest_ind(cool, warm, equal_var=True, permutations=10000, alternative='greater')\n",
    "print(\"T-test p-value = {}\".format(t_stat.pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fe5a9-1b2e-4ad8-8c74-cbf7558fb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate null distribution for permutation test histogram\n",
    "permutations = 10000 \n",
    "perm_mean = np.zeros((permutations))\n",
    "\n",
    "for i in range(permutations):\n",
    "    new_cool_mean = np.mean(np.random.permutation(cool)[0])\n",
    "    new_warm_mean = np.mean(np.random.permutation(warm)[0])\n",
    "    perm_mean[i] = new_cool_mean - new_warm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8309c-9289-406d-a950-bf26d0ad3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence interval\n",
    "alpha = 1\n",
    "CI99 = np.percentile(perm_mean, [alpha, 100-alpha]) # Not divided by 2 since one-sided test\n",
    "\n",
    "# Plot histogram of null distribution\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(perm_mean, bins=np.arange(math.floor(min(perm_mean)), math.ceil(max(perm_mean)), 0.5))\n",
    "\n",
    "# 99% CI Lower bound\n",
    "plt.plot([CI99[0], CI99[0]], [0, 6500], 'r', label='99% CI')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('mean(cool) - mean(warm)', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Histogram of 10,000 Permutations for the O-ring failure data', fontsize=16)\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.savefig('4.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436c6ad",
   "metadata": {},
   "source": [
    "## Task 5 Cloud Seeding Experiment\n",
    "Hypothesis is that seeded conditions will result in more rainfall compared to unseeded conditions.\\\n",
    "Alternative hypothesis: mean(seeded)-mean(unseeded)>0 \\\n",
    "Null hypothesis: mean(seeded)-mean(unseeded)≤0\n",
    "\n",
    "Hence, have one-sided two-sample t-test as comparing the performance of two different groups in the same environment and hypothesis only interested in seeding increasing rainfall, not reducing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0339316-970f-4fb8-b99c-e8420bb30416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read seeding data and check for NaN and incorrect values\n",
    "cloud_df = pd.read_excel(\"Cloud_Seeding_Case_Study.XLS\")\n",
    "print('Any temp NaN values? (True/False):', cloud_df.isnull().values.any())  # check nan values\n",
    "\n",
    "allowed_groups = ['SEEDED', 'UNSEEDED']\n",
    "print('Only allowed TREATMENT group names? (True/False):', \n",
    "      all(s in allowed_groups for s in cloud_df.loc[:, 'TREATMENT']))  # check allowed treatment groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0dcb0d-7a9b-41a6-89a4-e85b15399478",
   "metadata": {},
   "source": [
    "### Task 5.1 Parametric method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872eac5-d10d-4f0a-a488-ca239e2ba680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into the two groups + get mean\n",
    "unseeded = cloud_df[np.logical_not(cloud_df.loc[:,'TREATMENT']!='UNSEEDED')]\n",
    "unseeded = unseeded.loc[:, 'RAINFALL']\n",
    "unseeded_mean = np.mean(unseeded)\n",
    "print('Mean rain for unseeded conditions (mm):', np.round(unseeded_mean, 3))\n",
    "\n",
    "seeded = cloud_df[np.logical_not(cloud_df.loc[:,'TREATMENT']!='SEEDED')]\n",
    "seeded = seeded.loc[:, 'RAINFALL'].reset_index(drop=True)\n",
    "seeded_mean = np.mean(seeded)\n",
    "print('Mean rain for seeded conditions (mm):  ', np.round(seeded_mean, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a044a-dc96-48ee-9643-fc4b68a68681",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat = stats.ttest_ind(seeded, unseeded, equal_var=True, permutations=None, alternative='greater')\n",
    "print(\"T-test p-value = {}\".format(t_stat.pvalue)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d262e-f83b-4a7e-ba1d-8b52ee34e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95 and 99% confidence interval Lower bound\n",
    "var_seeded = np.var(seeded, ddof=1)  # Sample variance\n",
    "var_unseeded = np.var(unseeded, ddof=1)\n",
    "\n",
    "SE = np.sqrt(var_seeded / 26 + var_unseeded / 26)  # Standard error\n",
    "mean_difference = seeded_mean -unseeded_mean\n",
    "\n",
    "t_stat = mean_difference / SE  # t-value\n",
    "\n",
    "# Lower bound confidence intervals\n",
    "CI95 = mean_difference - t.ppf(0.95,50)*SE\n",
    "CI99 = mean_difference - t.ppf(0.99,50)*SE\n",
    "\n",
    "# Print result\n",
    "print('Lower bound 95% CI:', np.round(CI95, 3))\n",
    "print('Lower bound 99% CI:', np.round(CI99, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1ccbe-3199-4fb4-a240-76df3f21f763",
   "metadata": {},
   "source": [
    "### Task 5.2 Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f54a4-463a-40b1-a692-d2d595b9b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'greater' indicates one-sided test expecting seeded rainfall > unseeded rainfall\n",
    "t_stat_permutation = stats.ttest_ind(seeded, unseeded, equal_var=True, permutations=10000, alternative='greater')  \n",
    "print(\"T-test pvalue = {}\".format(t_stat_permutation.pvalue)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f83519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate null distribution for permutation test histogram\n",
    "permutations = 10000 \n",
    "perm_mean = np.zeros((permutations))\n",
    "\n",
    "for i in range(permutations):\n",
    "    new_seeded_mean = np.mean(np.random.permutation(seeded)[0])\n",
    "    new_unseeded_mean = np.mean(np.random.permutation(unseeded)[0])\n",
    "    perm_mean[i] = new_seeded_mean - new_unseeded_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60baffc-9df4-4530-ba34-e2c768cab2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence interval\n",
    "alpha99 = 1\n",
    "alpha95 = 5\n",
    "CI99 = np.percentile(perm_mean, [alpha99, 100-alpha99])  # Not divided by 2 since one-sided test\n",
    "CI95 = np.percentile(perm_mean, [alpha95, 100-alpha95])\n",
    "\n",
    "# Plot histogram with null distribution\n",
    "plt.figure(figsize=(9,8))\n",
    "plt.hist(perm_mean, bins=np.arange(math.floor(min(perm_mean)), math.ceil(max(perm_mean)), 50))\n",
    "\n",
    "# 99% CI Lower bound\n",
    "plt.plot([CI99[0], CI99[0]], [0, 1000], 'r', label='99% CI')\n",
    "\n",
    "# 95% CI Lower bound\n",
    "plt.plot([CI95[0], CI95[0]], [0, 1000], 'black', label='95% CI')\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('mean(seeded) - mean(unseeded)', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Histogram of 10,000 Permutations for the Cloud Seeding Data', fontsize=16)\n",
    "plt.legend(loc=1, fontsize=14)\n",
    "plt.savefig('5.2.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609f7f9-77d7-4a26-af8e-48d7ebc4cf63",
   "metadata": {},
   "source": [
    "### Task 5.3 Log-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039dff7-c5dc-4072-aa19-075bf2be8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform rainfall data\n",
    "seeded_log = np.log(seeded)\n",
    "unseeded_log = np.log(unseeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1c910-783e-4edf-9a06-d10a6414e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat = stats.ttest_ind(seeded_log, unseeded_log, equal_var=True, alternative='greater')\n",
    "print(\"T-test pvalue = {}\".format(t_stat.pvalue)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9081b-1ae6-4dd4-98dd-c670d457e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95 and 99% confidence interval Lower bound\n",
    "var_seeded_log = np.var(seeded_log, ddof=1)  # Sample variance\n",
    "var_unseeded_log = np.var(unseeded_log, ddof=1)\n",
    "\n",
    "SE = np.sqrt(var_seeded_log / 26 + var_unseeded_log / 26)  # Standard error\n",
    "mean_difference = np.mean(seeded_log) - np.mean(unseeded_log)\n",
    "\n",
    "t_stat = mean_difference / SE  # t-value\n",
    "\n",
    "# Lower bound confidence intervals\n",
    "CI95 = mean_difference - t.ppf(0.95,50)*SE\n",
    "CI99 = mean_difference - t.ppf(0.99,50)*SE\n",
    "\n",
    "# Print result\n",
    "print('Lower bound 95% CI:', np.round(CI95, 3))\n",
    "print('Lower bound 99% CI:', np.round(CI99, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ede2e",
   "metadata": {},
   "source": [
    "## Task 6 Exploratory Data Analysis and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab60931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Nitrogen and Flow csv data\n",
    "Ndata = pd.read_csv('Q_TKN_data.csv')\n",
    "Ndata = Ndata.rename(columns={\"Q (mm/d)\":\"Q\", \"TKN (mg/L)\":\"TKN\"})\n",
    "\n",
    "# Calculating Pearson and Spearman correlations (Raw Domain)\n",
    "N_person = stats.pearsonr(Ndata.Q, Ndata.TKN)\n",
    "N_spearman = stats.spearmanr(Ndata.Q, Ndata.TKN)\n",
    "print(\"Pearson Correlation Coefficient = \",N_person[0])\n",
    "print(\"Spearman’s Rank Correlation Coefficient = \", N_spearman.correlation)\n",
    "\n",
    "# Calculating Pearson and Spearman correlations (Log Domain)\n",
    "N_log_person = stats.pearsonr(np.log(Ndata.Q), np.log(Ndata.TKN))\n",
    "N_log_spearman = stats.spearmanr(np.log(Ndata.Q), np.log(Ndata.TKN))\n",
    "print(\"Logged Data Pearson Correlation Coefficient = \",N_log_person[0])\n",
    "print(\"Logged Data Spearman’s Rank Correlation Coefficient = \", N_log_spearman.correlation)\n",
    "\n",
    "# Plotting Raw and Logged Q vs TKN\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "ax1.scatter(Ndata.Q, Ndata.TKN, c='r')\n",
    "ax1.set_xlabel('Q')\n",
    "ax1.set_ylabel('TKN')\n",
    "ax2.scatter(np.log(Ndata.Q), np.log(Ndata.TKN), c='b')\n",
    "ax2.set_xlabel('log(Q)')\n",
    "ax2.set_ylabel('log(TKN)')\n",
    "plt.suptitle('Raw vs Log')\n",
    "plt.show()\n",
    "\n",
    "# Log Space has a better correlation, thus using log from now on...\n",
    "# Linear Regression (using log transformed data)\n",
    "N_slope, N_intercept, N_r, N_p, N_se = stats.linregress(np.log(Ndata.Q), np.log(Ndata.TKN))\n",
    "\n",
    "# predicting TKN (mg/L) at Q = 2 mm/d\n",
    "TKN_log_predicted = (N_slope * np.log(2)) + N_intercept # log(y) = m*log(x) + c\n",
    "TKN_predicted = np.exp(TKN_log_predicted) # transforming back to original domain\n",
    "\n",
    "# Plotting Linear Regression in Log-Space\n",
    "plt.figure()\n",
    "plt.scatter(np.log(Ndata.Q), np.log(Ndata.TKN), c='b')\n",
    "plt.plot(np.log(Ndata.Q), N_slope*np.log(Ndata.Q)+ N_intercept, 'k-')\n",
    "plt.xlabel('log(Q)')\n",
    "plt.ylabel('log(TKN)')\n",
    "plt.text(-5,0.8, s=\"r = \" + str(np.round(N_r,4)))\n",
    "plt.text(-5,0.7, s=\"log(TKN) = \" + str(np.round(N_slope,4)) + \"* log(Q) + \" + str(np.round(N_intercept,4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_N = len(Ndata)\n",
    "log_N_data = np.log(Ndata.Q)\n",
    "mean_N = np.mean(log_N_data)\n",
    "Yhat = N_slope * log_N_data + N_intercept\n",
    "\n",
    "SE_CI_95 = N_se * np.sqrt(1/len_N + ((log_N_data - mean_N)**2 / np.sum((log_N_data - mean_N)**2)))\n",
    "SE_PredCI_95 = N_se  * np.sqrt( 1 + 1/len_N + ((log_N_data - mean_N)**2 / np.sum((log_N_data - mean_N)**2)))\n",
    "\n",
    "# Plot the regression line with 95% CI of predicted values and 95% CI of conditional mean\n",
    "t_dist  = stats.t(len_N-2)\n",
    "\n",
    "# Add regression line to scatter plot \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.log(Ndata.Q), np.log(Ndata.TKN), s = 10, color='orange', alpha=0.6, edgecolor='red', linewidth=1)\n",
    "plt.xlabel('log(Q)')\n",
    "plt.ylabel('log(TKN)')\n",
    "plt.title('Linear Regression of Q vs TKN for Curdies River at Curdie')\n",
    "plt.plot(log_N_data, Yhat, lw = 1, c = 'k', label = 'Regression line') #regression line plotting from sklearn \n",
    "\n",
    "# Prediction CI\n",
    "plt.plot(log_N_data, Yhat + t_dist.ppf(0.975)*SE_PredCI_95, c = 'navy', lw=1, label = '95% CI of Predictions') #upper limit\n",
    "plt.plot(log_N_data, Yhat + t_dist.ppf(0.025)*SE_PredCI_95, c = 'navy', lw=1, label = None) #lower limit\n",
    "\n",
    "# Conditional mean CI\n",
    "plt.plot(log_N_data, Yhat + t_dist.ppf(0.975)*SE_CI_95, c = 'red', lw=1, label = '95% CI of Conditinonal Mean') #upper limit\n",
    "plt.plot(log_N_data, Yhat + t_dist.ppf(0.025)*SE_CI_95, c = 'red', lw=1, label = None) #lower limit\n",
    "plt.legend(loc = 0, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals and Autocorrelation\n",
    "tkn_resid = np.log(Ndata.TKN) - Yhat\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(np.log(Ndata.Q), tkn_resid, s=5)\n",
    "plt.plot([np.log(Ndata.Q).min(), np.log(Ndata.Q).max()],[0,0], 'k--')\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('log(Q) mm/d')\n",
    "plt.ylabel('Residuals (Observed TKN - Predicted TKN)')\n",
    "plt.savefig('Residuals_QvsTKN.png', dpi=360)\n",
    "plt.show()\n",
    "\n",
    "# Correlelogram of Residuals\n",
    "maxlags = 30\n",
    "plt.figure()\n",
    "plt.title(\"Autocorrelation Plot\")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.acorr(tkn_resid, maxlags=maxlags)\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,maxlags)\n",
    "plt.savefig('Correlogram.png', dpi=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of values insude the 95% Prediction CI\n",
    "count = 0\n",
    "predictions_upper = Yhat + t_dist.ppf(0.975)*SE_PredCI_95\n",
    "predictions_lower = Yhat + t_dist.ppf(0.025)*SE_PredCI_95\n",
    "\n",
    "for i in range(len(Ndata)):\n",
    "    if (np.log(Ndata.TKN[i]) > predictions_lower[i] and np.log(Ndata.TKN[i]) < predictions_upper[i]):\n",
    "        count += 1\n",
    "\n",
    "print(\"There were {} data points inside the prediction intervals\".format(count))\n",
    "print(\"The percentage of observations inside prediction intervals = {}%\".format((count/len(Ndata))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replotting in Raw Domain\n",
    "q_raw = np.linspace(0,2,1000)\n",
    "raw_predic = np.exp((N_slope*np.log(q_raw))+ N_intercept)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(Ndata.Q, Ndata.TKN, c='b')\n",
    "plt.plot(q_raw,raw_predic, 'k-')\n",
    "plt.xlabel('Q mm/d')\n",
    "plt.ylabel('TKN mg/L')\n",
    "plt.title(\"Q vs TKN with Fitted Model at Curdies River (Raw Space)\")\n",
    "plt.savefig('Q_TKN_rawspace.png', dpi=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ee0ef",
   "metadata": {},
   "source": [
    "## Task 7 Atmospheric CO2 Concentration during Global Forced Confinement by COVID-19ced Confinement by COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50310ac7",
   "metadata": {},
   "source": [
    "Task 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading-in and Cleaning Dataframe\n",
    "co2 = pd.read_csv('monthly_in_situ_co2_mlo.csv', skiprows=54)\n",
    "\n",
    "# Remvoing Redundant Columns and Rows\n",
    "co2 = co2.drop([0,1])\n",
    "cols = [2,3,5,6,7,8,9]\n",
    "co2.drop(co2.columns[cols],axis=1,inplace=True)\n",
    "\n",
    "# Creating Pandas Datetime\n",
    "co2['day'] = 15\n",
    "co2 = co2.rename(columns={\"  Yr\": \"year\", \" Mn\":\"month\", \"     CO2\": \"CO2\"})\n",
    "co2['Date'] = pd.to_datetime(co2[['year', 'month', 'day']])\n",
    "\n",
    "# Remvoing Redundant Date Columns\n",
    "cols = [0,1,3]\n",
    "co2.drop(co2.columns[cols],axis=1,inplace=True)\n",
    "co2df = co2.set_index(\"Date\")\n",
    "co2df = co2df['1995-04-15':'2021-12-15']\n",
    "co2df['CO2'].mask(co2df['CO2'] == -99.99, np.nan, inplace=True) # removing missing values which are denoted by -99.99                                                          \n",
    "co2df['CO2'] = pd.to_numeric(co2df['CO2']) # converting to floats\n",
    "\n",
    "# Remove Monthly Seasonality\n",
    "co2df_noseason = co2df.groupby(co2df.index.month).transform(lambda g: g - g.mean()) \n",
    "co2df_noseason.plot()\n",
    "plt.ylabel('Observed CO2 - Monthly Seasonality')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Observed CO2 without Seasonality')\n",
    "plt.savefig('7.1A.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Polynomial Fitting\n",
    "X = co2df_noseason.index\n",
    "X = np.array(range(0,len(X)))\n",
    "Y = co2df_noseason.CO2\n",
    "trend = np.polyfit(x=X, y=Y, deg=2)\n",
    "\n",
    "# Predicted CO2 from Fitted Trend\n",
    "co2_hat = (X**2)*trend[0] + X*trend[1] + trend[2]\n",
    "\n",
    "# Plot C02 with Polynomial Trend\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(co2df_noseason.index, co2_hat, 'k-')\n",
    "plt.plot(co2df_noseason.index, co2df_noseason.CO2)\n",
    "plt.ylabel('Observed CO2 - Monthly Seasonality')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Observed CO2 without Seasonality and Fitted Polynomial Trend')\n",
    "plt.show()\n",
    "\n",
    "# Calculating Residuals (observed - predicted)\n",
    "co2df_noseason['Residuals'] = co2df_noseason['CO2'] - co2_hat\n",
    "\n",
    "# Plot Residuals\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.scatter(co2df_noseason.index, co2df_noseason.Residuals, s=5)\n",
    "plt.plot([co2df_noseason.index.min(), co2df_noseason.index.max()],[0,0], 'k--')\n",
    "plt.ylim(-2,2)\n",
    "plt.title('Polynomial Fit on Observed CO2 Concentration (ppm)')\n",
    "plt.ylabel('Residuals (Observed CO2 - Predicted)')\n",
    "plt.savefig('7.1B.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546c8b8",
   "metadata": {},
   "source": [
    "Task 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8002cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Confidence Intervals on Predicted Values\n",
    "N = len(co2_hat)\n",
    "X = co2df_noseason.CO2\n",
    "meanX = np.mean(X)\n",
    "N_se = np.std(X)/np.sqrt(N)\n",
    "Yhat = co2_hat\n",
    "SE_PredCI = N_se  * np.sqrt(1 + 1/N + ((X - meanX)**2 / np.sum((X-meanX)**2)))\n",
    "t_dist  = stats.t(N-2)\n",
    "\n",
    "# Whole Timeseries with CI of Predictions\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(co2df_noseason.index, co2_hat, 'k-')\n",
    "plt.plot(co2df_noseason.index, co2df_noseason.CO2)\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.975)*SE_PredCI, c = 'navy', lw=1, label = '95% CI of Predictions') #upper limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.025)*SE_PredCI, c = 'navy', lw=1, label = None) #lower limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.995)*SE_PredCI, c = 'green', lw=1, label = '99% CI of Predictions') #upper limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.005)*SE_PredCI, c = 'green', lw=1, label = None) #lower limit\n",
    "plt.legend()\n",
    "plt.ylabel('Observed CO2 - Monthly Seasonality')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Observed CO2 without Seasonality and Fitted Polynomial Trend')\n",
    "plt.show()\n",
    "\n",
    "# Zoomed in Figure (2019-2021)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(co2df_noseason.index, co2df_noseason.CO2, c='k')\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.975)*SE_PredCI, c = 'navy', lw=1, label = '95% CI of Predictions') #upper limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.025)*SE_PredCI, c = 'navy', lw=1, label = None) #lower limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.995)*SE_PredCI, c = 'green', lw=1, label = '99% CI of Predictions') #upper limit\n",
    "plt.plot(co2df_noseason.index, co2_hat + t_dist.ppf(0.005)*SE_PredCI, c = 'green', lw=1, label = None) #lower limit\n",
    "plt.axvspan('2020-04-15','2021-03-15', alpha=0.15, color='red')\n",
    "plt.xlim(Timestamp('2019-07-15 00:00:00'), Timestamp('2021-07-15 00:00:00'))\n",
    "plt.ylim(20,35)\n",
    "plt.text(x=Timestamp('2020-07-15 00:00:00'), y=32, s='COVID-19', c='r', size=12)\n",
    "plt.legend()\n",
    "plt.ylabel('Observed CO2 - Monthly Seasonality')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Observed CO2 without Seasonality and Fitted Polynomial Trend')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gassian Kernel Dessity\n",
    "\n",
    "covid_co2 = co2df_noseason['2020-04-15':'2021-03-15']\n",
    "no_covid_co2 = co2df_noseason.drop(covid_co2.index)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "no_covid_co2.Residuals.plot(kind='hist', bins=30,  color='k', ax=ax, alpha=0.25, label='Non-Covid')\n",
    "covid_co2.Residuals.plot(kind='hist', bins=30, color='r', ax=ax, alpha=0.85, label='Covid')\n",
    "plt.xlabel('Resdiuals (Observed - Predicted)')\n",
    "plt.title('Monthly CO2 Concentration Histogram (Covid vs Non-Covid)')\n",
    "plt.legend()\n",
    "plt.savefig('co2_histgram.png', dpi=360)\n",
    "plt.show()\n",
    "\n",
    "covid_co2_desnity = stats.gaussian_kde(np.array(covid_co2.CO2))\n",
    "no_covid_co2_desnity = stats.gaussian_kde(np.array(no_covid_co2.CO2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77bb2c",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56912ee1-e504-4134-940f-51dbe2ea5d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d60a22687f88ee9bd6e31f7045079ba080720a288a9e56b86916ad942ad11783"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
